# 机器学习

## 内容与核心

+ 监督学习
1. 线性模型
2. 线性和二次判别分析
3. 核岭回归
4. 支持向量机
5. 随机梯度下降
6. 最近邻
7. 高斯过程
8. 交叉分解
9. 朴素贝叶斯
10. 决策树
11. 集成：梯度提升、随机森林、bagging、投票、stacking
12. 多类和多输出算法
13. 特征选择
14. 半监督学习
15. 等渗回归
16. 概率校准
17. 神经网络模型（监督）


+ 无监督学习
1. 高斯混合模型
2. 流形学习
3. 聚类
4. 双聚类
5. 信号分解（矩阵分解问题）
6. 协方差估计
7. 新颖性和异常值检测
8. 密度估计
9. 神经网络模型（无监督）



## 一 监督模型

### 1.线性模型

+ 普通最小二乘法
+ 非负最小二乘法
+ 普通最小二乘复杂度
+ 岭回归和分类
+ 套索
+ 多任务套索
+ 弹性网络
+ 多任务弹性网络
+ 最小角回归
+ LARS 套索
+ 正交匹配追踪（OMP）
+ 贝叶斯回归
+ 逻辑回归
+ 广义线性模型
+ 随机梯度下降 - SGD
+ 感知器
+ 被动攻击算法
+ 稳健性回归：异常值和建模误差
+ 分位数回归
+ 多项式回归：利用基函数扩展线性模型

线性模型通过以下方式扩展核心思想：
1. 正则化：控制过拟合（L1/L2正则化）。
2. 优化算法：适应不同场景（闭式解、梯度下降、LARS）。
3. 概率框架：贝叶斯方法引入不确定性。
4. 损失函数：适应不同任务（均方误差、Log损失、Huber损失）。
5. 非线性扩展：多项式回归、基函数扩展（如径向基函数）。

#### 什么是线性模型？

####




## 1 机器学习基本概念
机器学习模型分为
+ 有监督学习模型
+ 无监督学习模型

###  1.1 有监督学习模型
有监督学习模型主要分为**单模型**和**集成学习**两种：
+ **单模型**使用一个独立算法处理数据来学习和预测。其特点为简单易懂，训练和预测过程透明，计算开销小、训练时间短，且易于调试定位问题。不过，它易受数据噪声和异常值影响。
+ **集成学习**则结合多个模型提升预测性能，增强鲁棒性。其优势在于能综合各模型长处，性能通常优于单模型，对噪声和异常值耐受性更好。但它也存在不足，实现与调试复杂，需管理多个模型及组合方式，训练和预测消耗更多计算资源与时间。

#### 1.1.1 单模型
- **线性模型**：
    - **线性回归**：用于建立因变量和自变量之间的线性关系，通过最小化误差平方和来确定模型参数，常用于预测数值型结果。
    - **逻辑回归**：虽名字有“回归”，实则用于分类问题，利用对数几率函数将线性回归结果转化为概率值，判断样本所属类别。 
    - **Lasso**：在回归分析中引入L1正则化，可对系数进行压缩，实现特征选择和防止过拟合。
    - **Ridge**：即岭回归，引入L2正则化，在回归系数估计中加入惩罚项，使系数估计更稳定，降低方差。 
- **k近邻**：基于实例的学习方法，给定测试样本，通过计算与训练集中样本的距离（如欧氏距离），找出k个最近邻样本，根据这些样本的类别（分类问题）或数值（回归问题）来预测测试样本的结果。 

- **决策树**：
    - **ID3**：以信息增益为准则选择划分属性构建决策树，每次选择使信息增益最大的属性进行分裂。
    - **C5.0**：在ID3基础上改进，使用信息增益比，还能处理连续属性和缺失值，可生成规则集。 
    - **CART**：分类回归树，既能用于分类（二叉树，基于基尼指数选择划分属性 ），也能用于回归（基于最小平方误差 ）。 
- **神经网络**：
    - **感知机**：最简单的神经网络模型，由输入层、输出层组成，通过权重调整学习输入和输出的映射关系，可解决线性可分问题。 
    - **神经网络**：通常指多层感知机，包含多个隐藏层，能学习复杂的非线性映射关系，通过反向传播算法更新权重。 
- **支持向量机**：
    - **线性可分**：在样本线性可分情况下，寻找能最大化样本间隔的超平面进行分类。 
    - **线性支持**：针对近似线性可分情况，引入松弛变量允许部分样本出错。 
    - **线性不可分**：通过核函数将样本映射到高维空间，使其线性可分，再寻找最优超平面。 

#### 1.1.2 集成学习
- **Boosting**：
    - **GBDT**：梯度提升决策树，基于梯度下降思想，每次迭代拟合残差的近似值（负梯度 ），不断构建新决策树来提升模型性能。 
    - **AdaBoost**：通过改变样本权重，让后续弱学习器更关注之前被误分类的样本，迭代训练多个弱学习器并加权组合成强学习器。 
    - **XGBoost**：对GBDT的优化，在目标函数中加入正则项控制模型复杂度，支持并行计算，训练效率高，广泛应用于数据挖掘竞赛等领域。 
    - **LightGBM**：采用直方图算法等优化策略，减少内存占用和计算量，支持更快的并行学习，在处理大规模数据时表现出色。 
    - **CatBoost**：能自动处理类别型特征，采用排序提升算法，减少梯度估计偏差，在准确性和鲁棒性方面有不错表现。 
- **Bagging**：
    - **随机森林**：基于Bagging集成学习方法，从原始训练集有放回抽样构建多个子集，每个子集训练一棵决策树，最终通过对多棵树的预测结果进行投票（分类）或平均（回归）得到最终结果，能有效降低方差，防止过拟合。 


### 1.2 无监督学习模型
**无监督学习模型中的两个主要类别：聚类和降维。**


#### 1.2.1 聚类
+ 聚类: 是一种无监督学习技术，用于将数据集中相似的数据点分组。
  + K-means: 一种基于质心的聚类算法，通过迭代优化质心位置以最小化每个点到其所在簇中心的距离。
  + 层次聚类: 创建层次关系的聚类方法，可分为自底向上的聚合层次聚类和自顶向下的分裂层次聚类。
  + 谱聚类: 利用图论、通过计算数据点之间的相似度矩阵以进行聚类的方法。

#### 1.2.2  降维
+ 降维: 是无监督学习中的另一种方法，用于减少数据集的特征维数，同时尽量保留数据的主要信息。
  + PCA (主成分分析): 通过线性变换将数据映射到新的坐标系中，新坐标系的轴（主成分）按数据方差大小排序，以保留最大方差信息。
  + SVD (奇异值分解): 一种矩阵因子分解技术，常用于数据降维和压缩。
  + LDA (线性判别分析): 尽管通常用作监督学习，其思路也被用作降维技术，通过最大化类间分隔和最小化类内分布。


### 1.3 什么是监督学习？什么是非监督学习？
**所有的回归算法和分类算法都属于监督学习。并且明确的给给出初始值，在训练集中有特征和标签，并且通过训练获得一个模型，在面对只有特征而没有标签的数据时，能进行预测。**


+ **监督学习**：通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如 分类。
+ **非监督学习**：直接对输入数据集进行建模，例如强化学习、K-means 聚类、自编码、受限波尔兹曼机。
+ **半监督学习**：综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。






### 1.4 分类，回归和聚类

以下是重新制作的表格，更加清晰有序。您可以根据需要进一步调整格式：

|        分类       |                                定义                                |                       算法                       |         案例          |
|:---------------:|:------------------------------------------------------------------:|:-------------------------------------------------:|:---------------------:|
|      **分类**     |         对离散随机变量进行建模预测的监督学习                      |  LR, SVM, KNN, 决策树, 随机森林, GBDT            |      垃圾邮件分类     |
|      **回归**     |       对连续随机变量进行建模预测的监督学习                       |  非线性回归, SVR (支持向量回归->可用径向基核 (RBF)), 随机森林 |      房价预测        |
|      **聚类**     |         基于数据的内部规律，寻找其属于不同族群的无监督学习  |  Kmeans, 层次聚类, GMM (高斯混合模型)            |                     |




### 1.5 生成模式 vs 判别模式


## 2.线性模型

### 2.1 线性回归

**原理**: 用线性函数拟合数据，用 MSE（均方差） 计算损失，然后用梯度下降法(GD)找到一组使 MSE 最小的权重。


#### 2.1.1 什么是回归？哪些模型可用于解决回归问题？

**指分析因变量和自变量之间关系**

+ 线性回归: 对异常值非常敏感
+ 多项式回归: 如果指数选择不当，容易过拟合。
+ 岭回归
+ Lasso回归
+ 弹性网络回归

#### 2.1.2 线性回归的损失函数为什么是均方差?



#### 2.1.3 什么是线性回归？什么时候使用它？

利用最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析.
+ 自变量与因变量呈直线关系;
+ 因变量符合正态分布;
+ 因变量数值之间独立;
+ 方差是否齐性。



**最小二乘和均方误差之间的关系**

![alt text](image.png)


#### 2.1.4什么是梯度下降？SGD的推导？


#### 2.1.5什么是最小二乘法（最小平方法）？



#### 2.1.6 常见的损失函数有哪些？
1. 0-1损失  
2. 均方差损失(MSE)  
3. 平均绝对误差(MAE)   
4. 分位数损失(Quantile Loss)
分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数；  
实现了分别用不同的系数控制高估和低估的损失，进而实现分位数回归。   
5. 交叉熵损失
6. 合页损失
一种二分类损失函数，SVM的损失函数本质： Hinge Loss + L2 正则化   
合页损失的公式如下：
![alt text](image-1.png)


#### 2.1.7 有哪些评估回归模型的指标？
![alt text](image-2.png)


#### 2.1.8 什么是正规方程？

#### 2.1.9 梯度下降法找到的一定是下降最快的方向吗？
不一定，它只是目标函数在当前的点的切平面上下降最快的方向。

在实际执行期中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到超线性的收敛速度。梯度下降类的算法的收敛速度一般是线性甚至次线性的（在某些带复杂约束的问题）。



### 2.2 LR

也称为"对数几率回归"。

1. 分类，经典的二分类算法！
2. LR的过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证这个求解的模型的好坏。
3. **Logistic 回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）**
4. 回归模型中，y 是一个定性变量，比如 y = 0 或 1，logistic 方法主要应用于研究某些事件发生的概率。
5. **LR的本质：极大似然估计**
6. LR的激活函数：**Sigmoid**
7. LR的代价函数：交叉熵


优点：    
1. 速度快，适合二分类问题
2. 简单易于理解，直接看到各个特征的权重
3. 能容易地更新模型吸收新的数据
缺点：  
对数据和场景的适应能力有局限性，不如决策树算法适应性那么强。   

LR中最核心的概念是 Sigmoid 函数，Sigmoid函数可以看成LR的激活函数。

**Regression 常规步骤：**
1. 寻找h函数（即预测函数）
2. 构造J函数（损失函数）
3. 想办法（迭代）使得J函数最小并求得回归参数（θ）









#### 2.2.1 为什么 LR 要使用 sigmoid 函数？
1. 广义模型推导所得 
2. 满足统计的最大熵模型 
3. 性质优秀，方便使用
（Sigmoid函数是平滑的，而且任意阶可导，一阶二阶导数可以直接由函数值得到不用进行求导，这在实现中很实用）

#### 2.2.2 为什么常常要做特征组合（特征交叉）？
LR模型属于线性模型，线性模型不能很好处理非线性特征，特征组合可以引入非线性特征，提升模型的表达能力。  
另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸，过拟合，所以基本特征+特征组合兼顾了全局和个性化。    



#### 2.2.3 LR和线性回归的关系
![alt text](image-3.png)

**共同点**：
![alt text](image-4.png)




#### 2.2.4 LR参数求解的优化方法？(机器学习中常用的最优化方法)
梯度下降法，随机梯度下降法，牛顿法，拟牛顿法（LBFGS，BFGS,OWLQN）


目的都是求解某个函数的极小值。




### 2.3 Lasso

![alt text](image-5.png)

### 2.4 Ridge

![alt text](image-6.png)



### 2.5 对比
![alt text](image-7.png)








## 3.验证方式



### 3.1 什么是过拟合？产生过拟合原因?
指模型在训练集上的效果很好,在测试集上的预测效果很差.
1. 数据有噪声
2. 训练数据不足，有限的训练数据
3. 训练模型过度导致模型非常复杂

### 3.2 如何避免过拟合问题？
![alt text](image-8.png)



### 3.3 什么是机器学习的欠拟合？
模型复杂度低或者数据集太小,对模型数据的拟合程度不高,因此模型在训练集上的效果就不好.



### 3.4 如何避免欠拟合问题？
1. 增加样本的数量
2. 增加样本特征的个数
3. 可以进行特征维度扩展
4. 减少正则化参数
5. 使用集成学习方法，如Bagging


### 3.5 什么是交叉验证？交叉验证的作用是什么？
将原始dataset划分为两个部分.一部分为训练集用来训练模型,另外一部分作为测试集测试模型效果.

1. 交叉验证是用来评估模型在新的数据集上的预测效果,也可以一定程度上减小模型的过拟合
2. 还可以从有限的数据中获取尽能多的有效信息。


### 3.6 交叉验证主要有哪几种方法？
1. 留出法:简单地将原始数据集划分为训练集,验证集,测试集三个部分.
2. k折交叉验证:(一般取5折交叉验证或者10折交叉验证)
3. LOO留一法: (只留一个样本作为数据的测试集,其余作为训练集)---只适用于较少的数据集
4. Bootstrap方法:(会引入样本偏差)

### 3.7 什么是K折交叉验证？
将原始数据集划分为k个子集，将其中一个子集作为验证集，其余k-1个子集作为训练集，如此训练和验证一轮称为一次交叉验证。
交叉验证重复k次，每个子集都做一次验证集，得到k个模型，加权平均k个模型的结果作为评估整体模型的依据。


### 3.8 如何在K折交叉验证中选择K？
k越大，不一定效果越好，而且越大的k会加大训练时间；
在选择k时，需要考虑最小化数据集之间的方差，比如对于2分类任务，采用2折交叉验证，即将原始数据集对半分，若此时训练集中都是A类别，验证集中都是B类别，则交叉验证效果会非常差。

### 3.9 网格搜索（GridSearchCV）
一种调优方法，在参数列表中进行穷举搜索，对每种情况进行训练，找到最优的参数。


### 3.10 随机搜素（RandomizedSearchCV）
从超参数空间中随机采样一定数量的组合进行评估，从中选出表现最好的一组。



### 3.11 区别
网格搜索（Grid Search）和随机搜索（Random Search）都是超参数调优 的常用方法，用于在机器学习模型中寻找最优的超参数组合。虽然它们的目标相同，但在实现方式、效率、适用场景等方面有明显区别。

![alt text](image-9.png)


## 4.分类

### 4.1 几个易混淆的概念
+ 准确率（Accuracy）
+ 精准率(Precision) 
+ 召回率(Recall) 


准确率是所有判断正确的样本占所有样本的比例；   
精确率是所有判断为正例的样本中真正的正例占所有判断为正例的样本的比例；   
召回率则是所有正例中被判断出来的比率。   

### 4.2 P-R曲线

![alt text](image-11.png)


横轴为**召回率(查全率)**，纵轴为**精准率(查准率)**;  
引入“平衡点”(BEP)来度量，表示“查准率=查全率”时的取值，值越大表明分类器性能越好。




### 4.3 F1-Score
![alt text](image-12.png)
准确率和召回率的权衡: 只有在召回率Recall和精确率  

Precision都高的情况下，F1 score才会很高，比BEP更为常用。   




## 5. 正则化

### 5.1 什么是正则化？如何理解正则化？
**定义:** 在损失函数后加上一个正则化项（惩罚项），其实就是常说的结构风险最小化策略，即损失函数 加上正则化。一般模型越复杂，正则化值越大。   

正则化项是用来对模型中某些参数进行约束，正则化的一般形式：     

**第一项是损失函数（经验风险），第二项是正则化项**   

公式可以看出，加上惩罚项后损失函数的值会增大，要想损失函数最小，惩罚项的值要尽可能的小，模型参数就要尽可能的小，这样就能减小模型参数，使得模型更加简单。    


### 5.2 为何要常对数据做归一化？
1. 归一化后加快的梯度下降对最优解的速度。
2. 归一化有可能提高精度。


### 5.3 归一化和标准化的区别
标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。
归一化的目的是方便比较，可以加快网络的收敛速度；标准化是将数据利用z-score（均值、方差）的方法转化为符合特定分布的数据，方便进行下一步处理，不为比较。


### 5.4 需要归一化的算法有哪些？这些模型需要归一化的主要原因？

**线性回归,逻辑回归，KNN，SVM，神经网络。** 

主要是因为特征值相差很大时，运用梯度下降，损失等高线是椭圆形，需要进行多次迭代才能达到最优点，如果进行归一化了，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要迭代次数较少。   



## 6. 决策树

### 6.1 定义
定义： 决策树就是一棵树，其中跟节点和内部节点是输入特征的判定条件，叶子结点就是最终结果。
损失函数：正则化的极大似然函数；
目标是 以损失函数为目标函数的最小化。
算法通常是一个 递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。
决策树量化纯度：
判断数据集“纯”的指标有三个：Gini指数、熵、错误率


### 6.2 决策树的数据split原理或者流程？
1. 将所有样本看做一个节点
2. 根据纯度量化指标.计算每一个特征的’纯度’,根据最不’纯’的特征进行数据划分
3. 重复上述步骤,知道每一个叶子节点都足够的’纯’或者达到停止条件

背诵：按照基尼指数、信息增益来选择特征，保证划分后纯度尽可能高。



### 6.3 构造决策树的步骤？
1. 特征选择
2. 决策树的生成（包含预剪枝）  ---- 只考虑局部最优
3. 决策树的剪枝（后剪枝）      ---- 只考虑全局最优



### 6.4 决策树算法中如何避免过拟合和欠拟合？
过拟合:选择能够反映业务逻辑的训练集去产生决策树;   
剪枝操作(前置剪枝和后置剪枝); K折交叉验证(K-fold CV)
欠拟合:增加树的深度,RF



### 6.5 决策树怎么剪枝？
分为预剪枝和后剪枝，**预剪枝**是在决策树的构建过程中加入限制，比如控制叶子节点最少的样本个数，提前停止；

后剪枝是在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作.   

剪枝的目的就是**防止过拟合**，是模型在测试数据上变现良好，更加鲁棒.

### 6.6 决策树的优缺点？
决策树的优点：
1. 决策树模型可读性好，具有描述性，有助于人工分析；
2. 效率高，决策树只需要一次性构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。


决策树的缺点：
1. 即使做了预剪枝，它也经常会过拟合，泛化性能很差。
2. 对中间值的缺失敏感；
3. ID3算法计算信息增益时结果偏向数值比较多的特征。


### 6.7 决策树和条件概率分布的关系？
决策树可以表示成给定条件下类的条件概率分布. 决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大.




## 7. KNN

### 7.1 定义
KNN算法的核心思想是在一个含未知样本的空间，可以根据样本最近的k个样本的数据类型来确定未知样本的数据类型。 该算法涉及的3个主要因素是：k值选择，距离度量，分类决策。


### 7.2 



















## 8. SVM支持向量机


- 第四章介绍了当两个类别线性可分时的最优分离超平面
- 本章讨论对不可分情况（类别重叠）的扩展
- 支持向量机(SVM)通过在转换后的特征空间构建线性边界来产生非线性边界
- 本章还介绍了Fisher线性判别分析(LDA)的泛化方法：
  - 灵活判别分析：以类似SVM的方式构建非线性边界
  - 惩罚判别分析：用于特征高度相关的信号和图像分类问题
  - 混合判别分析：用于处理形状不规则的类别

### 8.1 支持向量分类器(SVC)

#### 支持向量机基本原理
- 最优分界面准则：两类模式向量分开的间距最大
- 支持向量：位于边界上的向量（带圈的向量）
- SVC基于统计学习理论，实现最优分类方法

#### 两类分类规则
- 两类模式间隔的距离为2/||W||
- 为使间隔最大，应使||W||最小，等价于使||W||²最小
- 目标函数：最小化||W||²/2
- 约束条件：yi[W^T·Xi+b] ≥ 1，i=1,2,...,N

#### 最优解计算
- 构造Lagrange函数：L(W,b,λ) = (W^T·W)/2 - Σλi[yi(W^T·xi+b)-1]
- 求最优解需满足约束条件：
  1. W = Σλi·yi·xi
  2. Σλi·yi = 0
- 最优分界面的解权向量：W* = Σλi*·yi·xi
- 支持向量是λi* > 0的样本点

#### 对偶问题求解
- 将原始问题转化为对偶形式：Q(λ) = Σλi - (1/2)·ΣΣλi·λj·yi·yj·(xi^T·xj)
- 利用Lagrange优化方法求解函数的极大值，得到λi*和||W*||²
- 最优分类规则：f(x) = sgn(Σλi*·yi·xi^T·x + b*)
- 阈值b*的计算：b* = (1/2)[W*^T·xs(1) - W*^T·xs(-1)]

#### 松弛变量(Slack Variables)
- 当类别不可线性分离时，引入松弛变量ξi
- 目标函数修改为：最小化||W||²/2 + C·Σξi
- 约束条件变为：yi(W^T·xi+b) ≥ 1-ξi，ξi ≥ 0
- C是惩罚参数，控制误分类的惩罚程度
- ξi表示样本不满足约束的程度

#### 计算支持向量分类器
- Lagrange函数：L = (||W||²/2) + C·Σξi - Σαi[yi(W^T·xi+b)-(1-ξi)] - Σμiξi
- 对偶问题求解得到αi
- 分类函数：f(x) = sgn(W^T·x + b) = sgn(Σαi·yi·xi^T·x + b)

### 8.2 支持向量机和核函数

#### 非线性SVM
- 支持向量分类器在输入特征空间中找到线性边界
- 可以通过扩展特征空间使方法更灵活（如多项式或样条基函数扩展）
- 在扩展空间的线性边界对应原始空间的非线性边界

#### 核函数计算SVM
- Lagrange对偶函数：L(α) = Σαi - (1/2)·ΣΣαi·αj·yi·yj·K(xi,xj)
- 解函数：f(x) = sgn(Σαi·yi·K(xi,x) + b)
- 核函数：K(x,x') = <h(x),h(x')>，是一个对称正(半)定函数
- 常用核函数：
  - 线性核：K(x,x') = x^T·x'
  - 多项式核：K(x,x') = (1+x^T·x')^d
  - 径向基函数(RBF)核：K(x,x') = exp(-γ||x-x'||²)
  - 神经网络核：K(x,x') = tanh(κ1(x^T·x')+κ2)

#### SVM作为惩罚方法
- SVM可看作一种特殊的正则化方法
- 目标函数包括损失函数和正则化项

#### SVM与维度灾难
- SVM通过核函数技巧有效处理高维空间
- 在高维空间中较不受维度灾难影响

### 8.3 用于回归的支持向量机

#### 回归问题的SVM
- 对于回归问题，目标是找到一个带状区域包含所有样本点
- 允许模型输出和实际输出之间存在ε偏差
- 引入松弛变量ξi和ξi*，允许部分样本点落在ε-带外

#### 回归SVM的数学表示
- 目标函数：最小化||W||²/2 + C·Σ(ξi+ξi*)
- 约束条件：
  - yi - (W^T·xi+b) ≤ ε + ξi
  - (W^T·xi+b) - yi ≤ ε + ξi*
  - ξi, ξi* ≥ 0

#### 回归SVM的计算
- Lagrange函数包含原始变量和对偶变量
- 回归模型的解函数：f(x) = Σ(αi*-αi)·K(xi,x) + b
- 支持向量是αi > 0或αi* > 0的样本点

#### 回归与核函数
- 同分类问题类似，回归SVM也可使用核函数实现非线性回归
- 最终的回归函数：f(x) = Σ(αi*-αi)·K(xi,x) + b

#### 惩罚最小二乘准则
- SVM回归可视为一种特殊的惩罚最小二乘法
- 在损失函数中引入ε-insensitive损失
- SVM回归通过寻找满足特定约束条件下的最小化函数



## 9. 聚类分析

### 9.1 介绍

1. 聚类的本质:
- 将对象分组,使同组内对象相似,不同组间对象不同
- 组内距离(intra-cluster distances)最小化
- 组间距离(inter-cluster distances)最大化

2. 聚类的应用:
- 理解(Understanding):分组相关文档、基因蛋白、股票等
- 总结(Summarization):减少大数据集的规模

3. **聚类的类型**:
+ 基于划分（Partition-based）：K-means
+ 层次聚类（Hierarchical Clustering）：
  + 凝聚层次聚类（AGNES, Bottom-up）：从最细粒度（每个点自成一簇）开始，逐步合并相似的簇。
  + 分裂层次聚类（DIANA, Top-down）：从整体数据（所有点一个簇）开始，逐步拆分不相似的子簇。
+ 基于密度（Density-based）：DBSCAN
+ 模型基于（Model-based / Mixture Models）：高斯混合模型（Gaussian Mixture Model, GMM）



**重要问题：如何求距离？**
  1. Euclidean Distance（l2 norm）
   表达式为 $\sqrt{\sum_{i=1}^{n}(x_{i}-y_{i})^2}$
  2. Manhattan Distance（l1 norm）
   表达式为 $\sum_{i=1}^{n}|x_{i}-y_{i}|$
  3. Minkowski Distance（L-p norm）
   表达式为 $\sum_{i=1}^{n}|x_{i}-y_{i}|^{p}$
  4. chebyshev Distance（ l-infinity norm）
    表达式为 $\max_{i=1}^{n}|x_{i}-y_{i}|$
   


### 9.2 基于划分（Partition-based）

**代表方法**：
+ K-means
+ 变体k-means++、bi-kmeans、kernel k-means

**主要特点**：
+ 需要事先指定聚类数 $K$。
+ 通过迭代式地进行“分配样本到簇”与“更新簇中心点”或“更新样本原型”的过程来收敛。
+ 对球状或凸形聚类效果较好，但对于非球状或大小差异较大的簇，效果缺陷明显。
+ 适合大规模数据，计算开销相对较低，但**对初值和超参数（如K）较敏感**。
+ 对异常数据比较敏感。



#### 9.2.1  K-means
**需要的预设：k和初始中心点**

一般来说，初始中心点是随机生成的，且初始中心点的选取对聚类结果影响很大。

+ 聚类数目（K）的选择：
  1. **elbow方法** ：Elbow方法是基于聚类内部的“紧致度”衡量指标（如簇内平方误差和 $\text{SSE} = \sum_{k=1}^{K} \sum_{\mathbf{x} \in C_k} \|\mathbf{x} - \mathbf{\mu}_k \|^2$）来评估聚类效果；通常，该指标随着聚类数 $K$ 的增大而减小，但当 $K$ 超过某个合理值时，指标的下降幅度会突然变得缓慢，形成一处明显的“折点”，这就是该方法名称中“肘部（elbow）”的由来。
  2. **silhouette方法（轮廓系数法）** ：
  综合考虑了簇内相似度（凝聚性）和簇间相似度（可分离性）。对于每个样本 $\mathbf{x}_i$，它的轮廓系数 $s(\mathbf{x}_i)$ 定义为
  $$
    s(\mathbf{x}_i) = \frac{b(\mathbf{x}_i) - a(\mathbf{x}_i)}{\max\{a(\mathbf{x}_i),\; b(\mathbf{x}_i)\}}
  $$  
  + $s(\mathbf{x}_i)$ 的取值在 $[-1, 1]$ 之间：值越接近1，表明该样本与所在簇的距离远小于与其他簇的距离（聚类效果好）。值越接近-1，表明该样本被“分错”簇的可能性很大。
  > $a(\mathbf{x}_i)$ 表示该样本与其所在簇中其他样本的平均距离（簇内平均距离）。
  > $b(\mathbf{x}_i)$ 表示该样本与相邻簇（距离最近的其他簇）中样本的平均距离。
  3. **Gap Statistic**
  4. **Dunn Index**



#### 9.2.2  K-means++
k-means++是针对k-means中初始质心点选取的优化算法。

流程如下：
1. 随机选取一个数据点作为初始的聚类中心
2. 当聚类中心数量小于k时：计算每个数据点与当前已有聚类中心的最短距离，用 D(x)表示，这个值越大，表示被选取为下一个聚类中心的概率越大，最后使用轮盘法选取下一个聚类中心


#### 9.2.3  bi-Kmeans
bi-kmeans是针对kmeans算法会陷入局部最优的缺陷进行的改进算法。

流程如下：
1. 将所有点视为一个簇
2. 当簇的个数小于K时对每一个簇计算总误差，在给定的簇上面进行 k-means 聚类（k=2）计算将该簇一分为二之后的总误差
3. 选取使得误差最小的那个簇进行划分操作



### 9.3 基于密度（Density-based）
k-means算法对于凸性数据具有良好的效果，能够根据距离来讲数据分为球状类的簇，但对于非凸形状的数据点，这个时候就需要用到基于密度的聚类方法了

代表方法：
+ DBSCAN
+ OPTICS

**主要特点**：
+ 需要提前确定ε和M的值  
+ 不需要提前设置聚类的个数
+ 对初值选取敏感，对噪声不敏感
+ 对密度不均的数据聚合效果（DBSCAN不好，OPTICS较好）

#### 9.3.1 DBSCAN

流程如下：
1. 标记所有对象为unvisited  
2. 当有标记对象时  
    1. 随机选取一个unvisited对象 \( p \)  
    2. 标记 \( p \) 为visited  
    3. 如果 \( p \) 的 \( \varepsilon \) 邻域内至少有 \( M \) 个对象，则  
        1. 创建一个新的簇 \( C \)，并把 \( p \) 放入 \( C \) 中  
        2. 设 \( N \) 是 \( p \) 的 \( \varepsilon \) 邻域内的集合，对 \( N \) 中的每个点 \( p' \)  
            1. 如果点 \( p' \) 是unvisited  
                1. 标记 \( p' \) 为visited  
                2. 如果 \( p' \) 的 \( \varepsilon \) 邻域至少有 \( M \) 个对象，则把这些点添加到 \( N \)  
                3. 如果 \( p' \) 还不是任何簇的成员，则把 \( p' \) 添加到 \( C \)  
        3. 保存 \( C \)  
    4. 否则标记 \( p \) 为噪声 



**DBSCAN算法关键概念:**   
- ε-邻域:对象半径ε内的区域
- 核心点:ε-邻域内至少包含MinPts个对象的点
- 直接密度可达:从核心点出发能直接到达的点
- 密度可达:通过多个核心点间接到达的点
- 密度相连:两点可从同一点密度可达




#### 9.3.2 OPTICS
在DBSCAN算法中，使用了统一的值，当数据密度不均匀的时候，如果设置了较小的值，则较稀疏的cluster中的节点密度会小于
，会被认为是边界点而不被用于进一步的扩展；如果设置了较大的
值，则密度较大且离的比较近的cluster容易被划分为同一个cluster。


对于密度不均的数据选取一个合适的是很困难的，对于高维数据，由于维度灾难(Curse of dimensionality),ε的选取将变得更加困难。

### 9.4 层次聚类（Hierarchical Clustering）
前面介绍的几种算法确实可以在较小的复杂度内获取较好的结果，但是这几种算法却存在一个链式效应的现象，比如：A与B相似，B与C相似，那么在聚类的时候便会将A、B、C聚合到一起，但是如果A与C不相似，就会造成聚类误差，严重的时候这个误差可以一直传递下去。为了降低链式效应，这时候层次聚类就该发挥作用了。




**簇间相似度度量：**  
除了需要衡量对象之间的距离之外，有些聚类算法（如层次聚类）还需要衡量cluster之间的距离 
+ **single  linkage**：也叫最短距离法，定义两个簇之间的距离为两个簇中距离最近的两个样本点的距离。比如有簇 A 和簇 B，在 A 中找一个点，B 中找一个点，使这两个点距离最短，该距离就是两簇距离。优点是能识别细长形状的簇结构；缺点是易受噪声和离群点影响，可能形成链状聚类结果 。
+ **complete  linkage**：又称最长距离法，将两个簇之间的距离定义为两个簇中距离最远的两个样本点的距离。即找出簇 A 和簇 B 中距离最远的一对点，其距离为两簇距离。能产生紧凑的聚类结果，对噪声和离群点相对不敏感，但可能会合并距离较远的簇 。
+ **average  linkage**：计算两个簇中所有样本点对之间距离的平均值，以此作为簇间距离。综合考虑了簇内多个样本点的关系，聚类结果相对稳健，能避免单链接和全链接的极端情况 。
+ **ward  linkage**：基于方差分析思想，目标是最小化合并簇时增加的簇内离差平方和。每次选择合并使簇内总方差增加最小的两个簇。倾向于产生大小相似的簇，在数据分布较为均匀时效果较好 。


#### 9.4.1 凝聚层次聚类（AGNES, Bottom-up）
称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 cluster，每次按一定的准则将最相近的两个 cluster 合并生成一个新的 cluster，如此往复，直至最终所有的对象都属于一个 cluster。这里主要关注此类算法。




过程如下：
1. 初始时每个样本为一个 cluster，计算距离矩阵 D，其中元素$D_{ij}$为样本点$D_{i}$和$D_{j}$之间的距离；
2. 遍历距离矩阵 D，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 cluster 的编号，将这两个 cluster 合并为一个新的 cluster 并依据 cluster 距离度量方法更新距离矩阵 D（删除这两个 cluster 对应的行和列，并把由新 cluster 所算出来的距离向量插入 D 中），存储本次合并的相关信息；
3. 重复 2 的过程，直至最终只剩下一个 cluster 。 






#### 9.4.2 分割层次聚类（SINGLE, Complete, Average）
又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 cluster，每次按一定的准则将某个 cluster 划分为多个 cluster，如此往复，直至每个对象均是一个 cluster。
   




### 9.5 比较总结

|算法类型|适合的数据类型|抗噪点性能|聚类形状|算法效率|
| ---- | ---- | ---- | ---- | ---- |
|kmeans|混合型|较差|球形|很高|
|k-means++|混合型|一般|球形|较高|
|bi-kmeans|混合型|一般|球形|较高|
|DBSCAN|数值型|较好|任意形状|一般|
|OPTICS|数值型|较好|任意形状|一般|
|Agglomerative|混合型|较好|任意形状|较差| 















